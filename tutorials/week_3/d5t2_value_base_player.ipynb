{"cells":[{"cell_type":"markdown","metadata":{"execution":{},"id":"smbYeJRdL7Uf"},"source":["# Tutorial 2: Value-Based Player\n","\n","**Week 3, Day 5: Reinforcement Learning for Games**\n","\n","**By Neuromatch Academy**\n","\n","__Content creators:__ Mandana Samiei, Raymond Chua, Tim Lilicrap, Blake Richards\n","\n","__Content reviewers:__ Arush Tagade, Lily Cheng, Melvin Selim Atay, Kelson Shilling-Scrivo\n","\n","__Content editors:__ Melvin Selim Atay, Spiros Chavlis, Gunnar Blohm\n","\n","__Production editors:__ Namrata Bafna, Gagana B, Spiros Chavlis"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"TFaRK6yzL7Uv"},"source":["**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n","\n","<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"x0r1xTPML7VF"},"source":["---\n","# Tutorial Objectives\n","\n","In this tutorial, you will implement a value-based player. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","execution":{},"id":"3bSg7qBOL7VK"},"outputs":[],"source":["# @title Tutorial slides\n","\n","from IPython.display import IFrame\n","IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/r67j2/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"W3CuyCAeL7VS"},"source":["These are the slides for the videos in the tutorial. If you want to locally download the slides, click [here](https://osf.io/r67j2/download)."]},{"cell_type":"markdown","metadata":{"execution":{},"id":"gBTYLJR1L7VU"},"source":["---\n","# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","execution":{},"id":"suBAMvcyL7VX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659118925932,"user_tz":180,"elapsed":12943,"user":{"displayName":"Joao Pedro Carvalho Moreira","userId":"11144433980626047251"}},"outputId":"f4d947c7-7a5a-4c59-d8eb-43a602ec1f3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 46 kB 985 kB/s \n","\u001b[K     |████████████████████████████████| 86 kB 2.5 MB/s \n","\u001b[?25h  Building wheel for evaltools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# @title Install dependencies\n","!pip install coloredlogs --quiet\n","\n","!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n","from evaltools.airtable import AirtableForm\n","\n","# generate airtable form\n","atform = AirtableForm('appn7VdPRseSoMXEG', 'W3D5_T2', 'https://portal.neuromatchacademy.org/api/redirect/to/9c55f6cb-cdf9-4429-ac1c-ec44fe64c303')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{},"id":"Unbr7_EVL7Ve","executionInfo":{"status":"ok","timestamp":1659118927898,"user_tz":180,"elapsed":2146,"user":{"displayName":"Joao Pedro Carvalho Moreira","userId":"11144433980626047251"}}},"outputs":[],"source":["# Imports\n","import os\n","import time\n","import torch\n","import random\n","import logging\n","import coloredlogs\n","\n","import numpy as np\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from tqdm.notebook import tqdm\n","from pickle import Unpickler\n","\n","log = logging.getLogger(__name__)\n","coloredlogs.install(level='INFO')  # Change this to DEBUG to see more info."]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","execution":{},"id":"Ke4S4n1RL7Vi","executionInfo":{"status":"ok","timestamp":1659118927899,"user_tz":180,"elapsed":67,"user":{"displayName":"Joao Pedro Carvalho Moreira","userId":"11144433980626047251"}}},"outputs":[],"source":["# @title Set random seed\n","\n","# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n","\n","# For DL its critical to set the random seed so that students can have a\n","# baseline to compare their results to expected results.\n","# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n","\n","# Call `set_seed` function in the exercises to ensure reproducibility.\n","import random\n","import torch\n","\n","def set_seed(seed=None, seed_torch=True):\n","  \"\"\"\n","  Function that controls randomness. NumPy and random modules must be imported.\n","\n","  Args:\n","    seed : Integer\n","      A non-negative integer that defines the random state. Default is `None`.\n","    seed_torch : Boolean\n","      If `True` sets the random seed for pytorch tensors, so pytorch module\n","      must be imported. Default is `True`.\n","\n","  Returns:\n","    Nothing.\n","  \"\"\"\n","  if seed is None:\n","    seed = np.random.choice(2 ** 32)\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  if seed_torch:\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","  print(f'Random seed {seed} has been set.')\n","\n","\n","# In case that `DataLoader` is used\n","def seed_worker(worker_id):\n","  \"\"\"\n","  DataLoader will reseed workers following randomness in\n","  multi-process data loading algorithm.\n","\n","  Args:\n","    worker_id: integer\n","      ID of subprocess to seed. 0 means that\n","      the data will be loaded in the main process\n","      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n","\n","  Returns:\n","    Nothing\n","  \"\"\"\n","  worker_seed = torch.initial_seed() % 2**32\n","  np.random.seed(worker_seed)\n","  random.seed(worker_seed)"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","execution":{},"id":"aStJgc1pL7Vv","executionInfo":{"status":"ok","timestamp":1659118927900,"user_tz":180,"elapsed":66,"user":{"displayName":"Joao Pedro Carvalho Moreira","userId":"11144433980626047251"}}},"outputs":[],"source":["# @title Set device (GPU or CPU). Execute `set_device()`\n","# especially if torch modules used.\n","\n","# Inform the user if the notebook uses GPU or CPU.\n","\n","def set_device():\n","  \"\"\"\n","  Set the device. CUDA if available, CPU otherwise\n","\n","  Args:\n","    None\n","\n","  Returns:\n","    Nothing\n","  \"\"\"\n","  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","  if device != \"cuda\":\n","    print(\"WARNING: For this notebook to perform best, \"\n","        \"if possible, in the menu under `Runtime` -> \"\n","        \"`Change runtime type.`  select `GPU` \")\n","  else:\n","    print(\"GPU is enabled in this notebook.\")\n","\n","  return device"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{},"id":"LKYQnRBPL7V5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659118927900,"user_tz":180,"elapsed":65,"user":{"displayName":"Joao Pedro Carvalho Moreira","userId":"11144433980626047251"}},"outputId":"9a71b2ff-7a4c-4bf5-d3a2-bca5750aab78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random seed 2021 has been set.\n","GPU is enabled in this notebook.\n"]}],"source":["SEED = 2021\n","set_seed(seed=SEED)\n","DEVICE = set_device()"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","execution":{},"id":"9U9bkiqyL7V7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659119027780,"user_tz":180,"elapsed":12394,"user":{"displayName":"Joao Pedro Carvalho Moreira","userId":"11144433980626047251"}},"outputId":"9ca9b759-b1df-4ea8-e1fe-1c52aacdbc42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and unzipping the file... Please wait.\n","Download completed.\n","Add the nma_rl_games in the path and import the modules.\n"]}],"source":["# @title Download the modules\n","\n","# @markdown Run this cell!\n","\n","# @markdown Download from OSF. The original repo is https://github.com/raymondchua/nma_rl_games.git\n","\n","import os, io, sys, shutil, zipfile\n","from urllib.request import urlopen\n","\n","# download from github repo directly\n","#!git clone git://github.com/raymondchua/nma_rl_games.git --quiet\n","REPO_PATH = 'nma_rl_games'\n","\n","if os.path.exists(REPO_PATH):\n","  download_string = \"Redownloading\"\n","  shutil.rmtree(REPO_PATH)\n","else:\n","  download_string = \"Downloading\"\n","\n","zipurl = 'https://osf.io/kf4p9/download'\n","print(f\"{download_string} and unzipping the file... Please wait.\")\n","with urlopen(zipurl) as zipresp:\n","  with zipfile.ZipFile(io.BytesIO(zipresp.read())) as zfile:\n","    zfile.extractall()\n","print(\"Download completed.\")\n","\n","print(f\"Add the {REPO_PATH} in the path and import the modules.\")\n","# add the repo in the path\n","sys.path.append('nma_rl_games/alpha-zero')\n","\n","# @markdown Import modules designed for use in this notebook\n","import Arena\n","\n","from utils import *\n","from Game import Game\n","from NeuralNet import NeuralNet\n","\n","from othello.OthelloLogic import Board"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","execution":{},"id":"TVmKs9BNL7V-","executionInfo":{"status":"ok","timestamp":1659119027784,"user_tz":180,"elapsed":35,"user":{"displayName":"Joao Pedro Carvalho Moreira","userId":"11144433980626047251"}}},"outputs":[],"source":["# @title Helper functions from previous tutorials\n","class OthelloGame(Game):\n","  \"\"\"\n","  Instantiate Othello Game\n","  \"\"\"\n","  square_content = {\n","      -1: \"X\",\n","      +0: \"-\",\n","      +1: \"O\"\n","      }\n","\n","  @staticmethod\n","  def getSquarePiece(piece):\n","    return OthelloGame.square_content[piece]\n","\n","  def __init__(self, n):\n","    self.n = n\n","\n","  def getInitBoard(self):\n","    # Return initial board (numpy board)\n","    b = Board(self.n)\n","    return np.array(b.pieces)\n","\n","  def getBoardSize(self):\n","    # (a,b) tuple\n","    return (self.n, self.n)\n","\n","  def getActionSize(self):\n","    # Return number of actions, n is the board size and +1 is for no-op action\n","    return self.n*self.n + 1\n","\n","  def getCanonicalForm(self, board, player):\n","    # Return state if player==1, else return -state if player==-1\n","    return player*board\n","\n","  def stringRepresentation(self, board):\n","    return board.tobytes()\n","\n","  def stringRepresentationReadable(self, board):\n","    board_s = \"\".join(self.square_content[square] for row in board for square in row)\n","    return board_s\n","\n","  def getScore(self, board, player):\n","    b = Board(self.n)\n","    b.pieces = np.copy(board)\n","    return b.countDiff(player)\n","\n","  @staticmethod\n","  def display(board):\n","    n = board.shape[0]\n","    print(\"   \", end=\"\")\n","    for y in range(n):\n","      print(y, end=\" \")\n","    print(\"\")\n","    print(\"-----------------------\")\n","    for y in range(n):\n","      print(y, \"|\", end=\"\")    # Print the row\n","      for x in range(n):\n","        piece = board[y][x]    # Get the piece to print\n","        print(OthelloGame.square_content[piece], end=\" \")\n","      print(\"|\")\n","    print(\"-----------------------\")\n","\n","  @staticmethod\n","  def displayValidMoves(moves):\n","      # Display possible moves\n","      A=np.reshape(moves[0:-1], board.shape)\n","      n = board.shape[0]\n","      print(\"  \")\n","      print(\"possible moves\")\n","      print(\"   \", end=\"\")\n","      for y in range(n):\n","        print(y, end=\" \")\n","      print(\"\")\n","      print(\"-----------------------\")\n","      for y in range(n):\n","        print(y, \"|\", end=\"\")    # Print the row\n","        for x in range(n):\n","          piece = A[y][x]    # Get the piece to print\n","          print(OthelloGame.square_content[piece], end=\" \")\n","        print(\"|\")\n","      print(\"-----------------------\")\n","\n","  def getNextState(self, board, player, action):\n","    \"\"\"\n","    Helper function to make valid move\n","    If player takes action on board, return next (board,player)\n","    and action must be a valid move\n","\n","    Args:\n","      board: np.ndarray\n","        Board of size n x n [6x6 in this case]\n","      player: Integer\n","        ID of current player\n","      action: np.ndarray\n","        Space of actions\n","\n","    Returns:\n","      (board,player) tuple signifying next state\n","    \"\"\"\n","    if action == self.n*self.n:\n","      return (board, -player)\n","    b = Board(self.n)\n","    b.pieces = np.copy(board)\n","    move = (int(action/self.n), action%self.n)\n","    b.execute_move(move, player)\n","    return (b.pieces, -player)\n","\n","  def getValidMoves(self, board, player):\n","    \"\"\"\n","    Helper function to make valid move\n","    If player takes action on board, return next (board,player)\n","    and action must be a valid move\n","\n","    Args:\n","      board: np.ndarray\n","        Board of size n x n [6x6 in this case]\n","      player: Integer\n","        ID of current player\n","      action: np.ndarray\n","        Space of action\n","\n","    Returns:\n","      valids: np.ndarray\n","        Returns a fixed size binary vector\n","    \"\"\"\n","    valids = [0]*self.getActionSize()\n","    b = Board(self.n)\n","    b.pieces = np.copy(board)\n","    legalMoves =  b.get_legal_moves(player)\n","    if len(legalMoves)==0:\n","      valids[-1]=1\n","      return np.array(valids)\n","    for x, y in legalMoves:\n","      valids[self.n*x+y]=1\n","    return np.array(valids)\n","\n","  def getGameEnded(self, board, player):\n","    \"\"\"\n","    Helper function to signify if game has ended\n","\n","    Args:\n","      board: np.ndarray\n","        Board of size n x n [6x6 in this case]\n","      player: Integer\n","        ID of current player\n","\n","    Returns:\n","      0 if not ended, 1 if player 1 won, -1 if player 1 lost\n","    \"\"\"\n","    b = Board(self.n)\n","    b.pieces = np.copy(board)\n","    if b.has_legal_moves(player):\n","      return 0\n","    if b.has_legal_moves(-player):\n","      return 0\n","    if b.countDiff(player) > 0:\n","      return 1\n","    return -1\n","\n","  def getSymmetries(self, board, pi):\n","    \"\"\"\n","    Get mirror/rotational configurations of board\n","\n","    Args:\n","      board: np.ndarray\n","        Board of size n x n [6x6 in this case]\n","      pi: np.ndarray\n","        Dimension of board\n","\n","    Returns:\n","      l: list\n","        90 degree of board, 90 degree of pi_board\n","    \"\"\"\n","    assert(len(pi) == self.n**2+1)  # 1 for pass\n","    pi_board = np.reshape(pi[:-1], (self.n, self.n))\n","    l = []\n","\n","    for i in range(1, 5):\n","      for j in [True, False]:\n","        newB = np.rot90(board, i)\n","        newPi = np.rot90(pi_board, i)\n","        if j:\n","          newB = np.fliplr(newB)\n","          newPi = np.fliplr(newPi)\n","        l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n","    return l\n","\n","class RandomPlayer():\n","  \"\"\"\n","  Simulates Random Player\n","  \"\"\"\n","\n","  def __init__(self, game):\n","    self.game = game\n","\n","  def play(self, board):\n","    \"\"\"\n","    Simulates game play\n","\n","    Args:\n","      board: np.ndarray\n","        Board of size n x n [6x6 in this case]\n","\n","    Returns:\n","      a: int\n","        Randomly chosen move\n","    \"\"\"\n","\n","    # Compute the valid moves using getValidMoves()\n","    valids = self.game.getValidMoves(board, 1)\n","\n","    # Compute the probability of each move being played (random player means this should\n","    # be uniform for valid moves, 0 for others)\n","    prob = valids/valids.sum()\n","\n","    # Pick an action based on the probabilities (hint: np.choice is useful)\n","    a = np.random.choice(self.game.getActionSize(), p=prob)\n","\n","    return a"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"2hl9fOuiL7WB"},"source":["The hyperparameters used throughout the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{},"id":"ekPF7qshL7WE"},"outputs":[],"source":["args = dotdict({\n","    'numIters': 1,            # In training, number of iterations = 1000 and num of episodes = 100\n","    'numEps': 1,              # Number of complete self-play games to simulate during a new iteration.\n","    'tempThreshold': 15,      # To control exploration and exploitation\n","    'updateThreshold': 0.6,   # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n","    'maxlenOfQueue': 200,     # Number of game examples to train the neural networks.\n","    'numMCTSSims': 15,        # Number of games moves for MCTS to simulate.\n","    'arenaCompare': 10,       # Number of games to play during arena play to determine if new net will be accepted.\n","    'cpuct': 1,\n","    'maxDepth':5,             # Maximum number of rollouts\n","    'numMCsims': 5,           # Number of monte carlo simulations\n","    'mc_topk': 3,             # Top k actions for monte carlo rollout\n","\n","    'checkpoint': './temp/',\n","    'load_model': False,\n","    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n","    'numItersForTrainExamplesHistory': 20,\n","\n","    # Define neural network arguments\n","    'lr': 0.001,               # lr: Learning Rate\n","    'dropout': 0.3,\n","    'epochs': 10,\n","    'batch_size': 64,\n","    'device': DEVICE,\n","    'num_channels': 512,\n","})"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"t2IyCILTL7WH"},"source":["---\n","# Section 1: Train a value function from expert game data\n","\n","*Time estimate: ~25mins*\n","\n"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"Yvd-dM83L7WK"},"source":["Now that we have the game set up and working, we can build a (hopefully) smarter player by learning a value function using expert game data. Our player can then use this value function to decide what moves to make.\n","\n","**Goal:** Learn how to train a value function from a dataset of games played by an expert.\n","\n","**Exercise:** \n","\n","* Load a dataset of expert generated games.\n","* Train a network to minimize MSE for win/loss predictions given board states sampled throughout the game. This will be done on a very small number of games. We will provide a network trained on a larger dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","execution":{},"id":"OdhCrTozL7WL"},"outputs":[],"source":["# @title Video 1: Train a value function\n","from ipywidgets import widgets\n","\n","out2 = widgets.Output()\n","with out2:\n","  from IPython.display import IFrame\n","  class BiliVideo(IFrame):\n","    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n","      self.id=id\n","      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n","      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n","\n","  video = BiliVideo(id=f\"BV1pg411j7f7\", width=854, height=480, fs=1)\n","  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n","  display(video)\n","\n","out1 = widgets.Output()\n","with out1:\n","  from IPython.display import YouTubeVideo\n","  video = YouTubeVideo(id=f\"f9lZq0WQJFg\", width=854, height=480, fs=1, rel=0)\n","  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n","  display(video)\n","\n","out = widgets.Tab([out1, out2])\n","out.set_title(0, 'Youtube')\n","out.set_title(1, 'Bilibili')\n","\n","# add event to airtable\n","atform.add_event('Video 1: Train a value function')\n","\n","display(out)"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"zs-hFMumL7WN"},"source":["## Section 1.1. Load expert data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{},"id":"wXJrt0q4L7WP"},"outputs":[],"source":["def loadTrainExamples(folder, filename):\n","  \"\"\"\n","  Helper function to load Training examples\n","\n","  Args:\n","    folder: string\n","      Path specifying training examples\n","    filename: string\n","      File name of training examples\n","\n","  Returns:\n","    trainExamplesHistory: list\n","      Returns examples based on the model were already collected (loaded)\n","  \"\"\"\n","  trainExamplesHistory = []\n","  modelFile = os.path.join(folder, filename)\n","  examplesFile = modelFile + \".examples\"\n","  if not os.path.isfile(examplesFile):\n","    print(f'File \"{examplesFile}\" with trainExamples not found!')\n","    r = input(\"Continue? [y|n]\")\n","    if r != \"y\":\n","      sys.exit()\n","  else:\n","    print(\"File with train examples found. Loading it...\")\n","    with open(examplesFile, \"rb\") as f:\n","      trainExamplesHistory = Unpickler(f).load()\n","    print('Loading done!')\n","    return trainExamplesHistory"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{},"id":"aQ_bWHmvL7WU"},"outputs":[],"source":["path = \"nma_rl_games/alpha-zero/pretrained_models/data/\"\n","loaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"oojpv0g7L7WV"},"source":["## Section 1.2. Define the Neural Network Architecture for Othello\n"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"PM-bu-qTL7WY"},"source":["We will (somewhat arbitrarily) use a deep CNN with 4 convolutional layers and 4 linear layers with ReLU transfer functions and batch normalization.  One reason why convolutions are interesting here is because they can extract the local value of moves on the board regardless of board position; convolution would thus be able to extract the translation-invariant aspects of the play.\n","\n","For the Value Network network, the 3rd linear layer represents the policy and the 4th linear layer (output) represents the value function. The value function is a weighted sum over all policies. \n","\n","We can do this by assuming that the weights between linear layers 3 and 4 approximate the value-action function $w_{l_{34}}=Q^{\\pi}(s,a)$ in:\n","\n","\\begin{equation}\n","V^{\\pi}(s) = \\sum_{a}{\\pi(a,s) \\cdot Q^{\\pi}(s,a)}\n","\\end{equation}\n","\n","**Note**: `OthelloNet` has 2 outputs:\n","1. log-softmax of linear layer 3\n","2. tanh of linear layer 4"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"T_J2pMQCL7Wa"},"source":["<figure>\n","  <img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D5_ReinforcementLearningForGames/static/CNN.jpg\">\n","</figure>\n"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"w_dm9M5rL7Wb"},"source":["### Coding Exercise 1.2: Implement the NN `OthelloNNet` for Othello\n","\n","We implement most of OthelloNNet below but please complete the code to get the final outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{},"id":"tUXvTPMYL7Wd"},"outputs":[],"source":["class OthelloNNet(nn.Module):\n","  \"\"\"\n","  Instantiate Othello Neural Net with following configuration\n","  nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1) # Convolutional Layer 1\n","  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1, padding=1) # Convolutional Layer 2\n","  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1) # Convolutional Layer 3\n","  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1) # Convolutional Layer 4\n","  nn.BatchNorm2d(args.num_channels) X 4\n","  nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024) # Fully-connected Layer 1\n","  nn.Linear(1024, 512) # Fully-connected Layer 2\n","  nn.Linear(512, self.action_size) # Fully-connected Layer 3\n","  nn.Linear(512, 1) # Fully-connected Layer 4\n","  \"\"\"\n","\n","  def __init__(self, game, args):\n","    \"\"\"\n","    Initialise game parameters\n","\n","    Args:\n","      game: OthelloGame instance\n","        Instance of the OthelloGame class above;\n","      args: dictionary\n","        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n","        arena, checkpointing, and neural network parameters:\n","        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n","        num_channels: 512\n","\n","    Returns:\n","      Nothing\n","    \"\"\"\n","    self.board_x, self.board_y = game.getBoardSize()\n","    self.action_size = game.getActionSize()\n","    self.args = args\n","\n","    super(OthelloNNet, self).__init__()\n","    self.conv1 = nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1)\n","    self.conv2 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1,\n","                           padding=1)\n","    self.conv3 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n","    self.conv4 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n","\n","    self.bn1 = nn.BatchNorm2d(args.num_channels)\n","    self.bn2 = nn.BatchNorm2d(args.num_channels)\n","    self.bn3 = nn.BatchNorm2d(args.num_channels)\n","    self.bn4 = nn.BatchNorm2d(args.num_channels)\n","\n","    self.fc1 = nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024)\n","    self.fc_bn1 = nn.BatchNorm1d(1024)\n","\n","    self.fc2 = nn.Linear(1024, 512)\n","    self.fc_bn2 = nn.BatchNorm1d(512)\n","\n","    self.fc3 = nn.Linear(512, self.action_size)\n","\n","    self.fc4 = nn.Linear(512, 1)\n","\n","  def forward(self, s):\n","    \"\"\"\n","    Controls forward pass of OthelloNNet\n","\n","    Args:\n","      s: np.ndarray\n","        Array of size (batch_size x board_x x board_y)\n","\n","    Returns:\n","      Probability distribution over actions at the current state and the value of the current state.\n","    \"\"\"\n","    s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n","    s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n","    s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n","    s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n","    s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n","    s = s.view(-1, self.args.num_channels * (self.board_x - 4) * (self.board_y - 4))\n","\n","    s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n","    s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n","\n","    pi = self.fc3(s)  # batch_size x action_size\n","    v = self.fc4(s)   # batch_size x 1\n","    #################################################\n","    ## TODO for students: Compute the outputs of OthelloNNet in this order\n","    # 1. Log softmax of linear layer 3\n","    # 2. tanh of linear layer 4\n","    # Fill out function and remove\n","    raise NotImplementedError(\"Calculate the probability distribution and the value\")\n","    #################################################\n","    # Returns probability distribution over actions at the current state and the value of the current state.\n","    return ..., ...\n","\n","\n","# Add event to airtable\n","atform.add_event('Coding Exercise 1.2: Implement the NN OthelloNNet for Othello')"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"_W6WynigL7We"},"source":["[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D5_ReinforcementLearningForGames/solutions/W3D5_Tutorial2_Solution_fefbac77.py)\n","\n"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"ZtacJ9X3L7Wf"},"source":["## Section 1.3. Define the Value network\n","\n","Next we need to implement the training of the network we created above. We want to train it to approximate the value function - we will use real examples (the expert data from above) to train it.  So we need to specify the standard initialization, training, prediction and loss functions. \n","\n","**Note**: During training, the ground truth will be uploaded from the **MCTS simulations** available at `checkpoint_x.path.tar.examples`."]},{"cell_type":"markdown","metadata":{"execution":{},"id":"FSEjDwVWL7Wf"},"source":["### Coding Exercise 1.3: Implement the `ValueNetwork`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{},"id":"TXJLIvm7L7Wg"},"outputs":[],"source":["class ValueNetwork(NeuralNet):\n","  \"\"\"\n","  Initiates the Value Network\n","  \"\"\"\n","\n","  def __init__(self, game):\n","    \"\"\"\n","    Initialise network parameters\n","\n","    Args:\n","      game: OthelloGame instance\n","        Instance of the OthelloGame class above;\n","\n","    Returns:\n","      Nothing\n","    \"\"\"\n","    self.nnet = OthelloNNet(game, args)\n","    self.board_x, self.board_y = game.getBoardSize()\n","    self.action_size = game.getActionSize()\n","    self.nnet.to(args.device)\n","\n","  def train(self, games):\n","    \"\"\"\n","    Function to train value network\n","\n","    Args:\n","      games: list\n","        List of examples with each example is of form (board, pi, v)\n","\n","    Returns:\n","      Nothing\n","    \"\"\"\n","    optimizer = optim.Adam(self.nnet.parameters())\n","    for examples in games:\n","      for epoch in range(args.epochs):\n","        print('EPOCH ::: ' + str(epoch + 1))\n","        self.nnet.train()\n","        v_losses = []   # To store the losses per epoch\n","        batch_count = int(len(examples) / args.batch_size)  # len(examples)=200, batch-size=64, batch_count=3\n","        t = tqdm(range(batch_count), desc='Training Value Network')\n","        for _ in t:\n","          sample_ids = np.random.randint(len(examples), size=args.batch_size)  # Read the ground truth information from MCTS simulation using the loaded examples\n","          boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))  # Length of boards, pis, vis = 64\n","          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n","          target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n","\n","          # Predict\n","          # To run on GPU if available\n","          boards, target_vs = boards.contiguous().to(args.device), target_vs.contiguous().to(args.device)\n","\n","          #################################################\n","          ## TODO for students:\n","          ## 1. Compute the value predicted by OthelloNNet() ##\n","          ## 2. First implement the loss_v() function below and then use it to update the value loss. ##\n","          # Fill out function and remove\n","          raise NotImplementedError(\"Compute the output\")\n","          #################################################\n","          # Compute output\n","          _, out_v = ...\n","          l_v = ...  # Total loss\n","\n","          # Record loss\n","          v_losses.append(l_v.item())\n","          t.set_postfix(Loss_v=l_v.item())\n","\n","          # Compute gradient and do SGD step\n","          optimizer.zero_grad()\n","          l_v.backward()\n","          optimizer.step()\n","\n","  def predict(self, board):\n","    \"\"\"\n","    Function to perform prediction\n","\n","    Args:\n","      board: np.ndarray\n","        Board of size n x n [6x6 in this case]\n","\n","    Returns:\n","      v: OthelloNet instance\n","        Data of the OthelloNet class instance above;\n","    \"\"\"\n","    # Timing\n","    start = time.time()\n","\n","    # Preparing input\n","    board = torch.FloatTensor(board.astype(np.float64))\n","    board = board.contiguous().to(args.device)\n","    board = board.view(1, self.board_x, self.board_y)\n","    self.nnet.eval()\n","    with torch.no_grad():\n","        _, v = self.nnet(board)\n","    return v.data.cpu().numpy()[0]\n","\n","  def loss_v(self, targets, outputs):\n","    \"\"\"\n","    Calculates Mean squared error\n","\n","    Args:\n","      targets: np.ndarray\n","        Ground Truth variables corresponding to input\n","      outputs: np.ndarray\n","        Predictions of Network\n","\n","    Returns:\n","      MSE Loss calculated as: square of the difference between your model's predictions\n","      and the ground truth and average across the whole dataset\n","    \"\"\"\n","    #################################################\n","    ## TODO for students: Please compute Mean squared error and return as output. ##\n","    # Fill out function and remove\n","    raise NotImplementedError(\"Calculate the loss\")\n","    #################################################\n","    # Mean squared error (MSE)\n","    return ...\n","\n","  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n","    \"\"\"\n","    Code Checkpointing\n","\n","    Args:\n","      folder: string\n","        Path specifying training examples\n","      filename: string\n","        File name of training examples\n","\n","    Returns:\n","      Nothing\n","    \"\"\"\n","    filepath = os.path.join(folder, filename)\n","    if not os.path.exists(folder):\n","      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n","      os.mkdir(folder)\n","    else:\n","      print(\"Checkpoint Directory exists! \")\n","    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n","    print(\"Model saved! \")\n","\n","  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n","    \"\"\"\n","    Load code checkpoint\n","\n","    Args:\n","      folder: string\n","        Path specifying training examples\n","      filename: string\n","        File name of training examples\n","\n","    Returns:\n","      Nothing\n","    \"\"\"\n","    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n","    filepath = os.path.join(folder, filename)\n","    if not os.path.exists(filepath):\n","      raise (\"No model in path {}\".format(filepath))\n","\n","    checkpoint = torch.load(filepath, map_location=args.device)\n","    self.nnet.load_state_dict(checkpoint['state_dict'])\n","\n","\n","# Add event to airtable\n","atform.add_event('Coding Exercise 1.3: Implement the ValueNetwork')"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"_z-uUg6QL7Wq"},"source":["[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D5_ReinforcementLearningForGames/solutions/W3D5_Tutorial2_Solution_1ddaa5d8.py)\n","\n"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"p09xgaUJL7Wq"},"source":["## Section 1.4. Train the value network and observe the MSE loss progress\n","\n","**Important:** Run this cell ONLY if you do not have access to the pretrained models in the `rl_for_games` repository. The below cell will run the training algorithm and will take a while to complete...\n","\n","We provide a fully trained Value net in the `rl_for_games` repository that will automatically load below."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{},"id":"C-D2OW9SL7Wr"},"outputs":[],"source":["if not os.listdir('nma_rl_games/alpha-zero/pretrained_models/models/'):\n","  set_seed(seed=SEED)\n","  game = OthelloGame(6)\n","  vnet = ValueNetwork(game)\n","  vnet.train(loaded_games)"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"O8R8EfJ5L7Wr"},"source":["---\n","# Section 2: Use a trained value network to play games\n","\n","*Time estimate: ~25mins*\n"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"AO5IuJk4L7Ws"},"source":["Now that we have our value network all set up and trained, we're ready to test it by using it to play games.\n","\n","**Goal**: Learn how to use a value function in order to make a player that works better than a random player.\n","\n","**Exercise:**\n","* Sample random valid moves and use the value function to rank them\n","* Choose the best move as the action and play it\n","Show that doing so beats the random player\n","\n","**Hint:** You might need to change the sign of the value based on the player."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","execution":{},"id":"nEPuN7JwL7Ws"},"outputs":[],"source":["# @title Video 2: Play games using a value function\n","from ipywidgets import widgets\n","\n","out2 = widgets.Output()\n","with out2:\n","  from IPython.display import IFrame\n","  class BiliVideo(IFrame):\n","    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n","      self.id=id\n","      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n","      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n","\n","  video = BiliVideo(id=f\"BV1Ug411j7ig\", width=854, height=480, fs=1)\n","  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n","  display(video)\n","\n","out1 = widgets.Output()\n","with out1:\n","  from IPython.display import YouTubeVideo\n","  video = YouTubeVideo(id=f\"tvmzVHPBKKs\", width=854, height=480, fs=1, rel=0)\n","  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n","  display(video)\n","\n","out = widgets.Tab([out1, out2])\n","out.set_title(0, 'Youtube')\n","out.set_title(1, 'Bilibili')\n","\n","# add event to airtable\n","atform.add_event('Video 2: Play games using a value function')\n","\n","display(out)"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"4WHyt1_bL7Wt"},"source":["## Coding Exercise 2: Value-based player"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"u21gg3jWL7Wt"},"source":["Let's first initialize a new game and load in a pre-trained Value function."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{},"id":"3nHj_ko8L7Wu"},"outputs":[],"source":["model_save_name = 'ValueNetwork.pth.tar'\n","path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n","set_seed(seed=SEED)\n","game = OthelloGame(6)\n","vnet = ValueNetwork(game)\n","vnet.load_checkpoint(folder=path, filename=model_save_name)"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"O7itKh4RL7Wu"},"source":["Next, we can create a player that makes use of the value function to decide what best action to take next.\n","\n","How do we choose the best move using our value network? We will simply compute the expected value (predicted value) of all possible moves and then select the best one based on which next state has the highest value. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{},"id":"KvpF9Q7LL7Wv"},"outputs":[],"source":["class ValueBasedPlayer():\n","  \"\"\"\n","  Simulate Value Based Player\n","  \"\"\"\n","\n","  def __init__(self, game, vnet):\n","    \"\"\"\n","    Initialise value based player parameters\n","\n","    Args:\n","      game: OthelloGame instance\n","        Instance of the OthelloGame class above;\n","      vnet: Value Network instance\n","        Instance of the Value Network class above;\n","\n","    Returns:\n","      Nothing\n","    \"\"\"\n","    self.game = game\n","    self.vnet = vnet\n","\n","  def play(self, board):\n","    \"\"\"\n","    Simulate game play\n","\n","    Args:\n","      board: np.ndarray\n","        Board of size n x n [6x6 in this case]\n","\n","    Returns:\n","      candidates: List\n","        Collection of tuples describing action and values of future predicted states\n","    \"\"\"\n","    valids = self.game.getValidMoves(board, 1)\n","    candidates = []\n","    max_num_actions = 4\n","    va = np.where(valids)[0]\n","    va_list = va.tolist()\n","    random.shuffle(va_list)\n","    #################################################\n","    ## TODO for students: In the first part, please return the next board state using getNextState(), then predict\n","    ## the value of next state using value network, and finally add the value and action as a tuple to the candidate list.\n","    ## Note that you need to reverse the sign of the value. In zero-sum games the players flip every turn. In detail, we train\n","    ## a value function to think about the game from one player's (either black or white) perspective. In order to use the same\n","    ## value function to estimate how good the position is for the other player, we need to take the negative of the output of\n","    ## the function. E.g., if the value function is trained for white's perspective and says that white is likely to win the game\n","    ## from the current state with an output of 0.75, this similarly means that it would suggest that black is very unlikely (-0.75)\n","    ## to win the game from the current state.##\n","    # Fill out function and remove\n","    raise NotImplementedError(\"Implement the value-based player\")\n","    #################################################\n","    for a in va_list:\n","      # Return next board state using getNextState() function\n","      nextBoard, _ = ...\n","      # Predict the value of next state using value network\n","      value = ...\n","      # Add the value and the action as a tuple to the candidate lists, note that you might need to change the sign of the value based on the player\n","      candidates += ...\n","\n","      if len(candidates) == max_num_actions:\n","        break\n","\n","    # Sort by the values\n","    candidates.sort()\n","\n","    # Return action associated with highest value\n","    return candidates[0][1]\n","\n","\n","# Add event to airtable\n","atform.add_event('Coding Exercise 3: Value-based player')\n","\n","# Playing games between a value-based player and a random player\n","set_seed(seed=SEED)\n","num_games = 20\n","player1 = ValueBasedPlayer(game, vnet).play\n","player2 = RandomPlayer(game).play\n","arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n","## Uncomment the code below to check your code!\n","# result = arena.playGames(num_games, verbose=False)\n","# print(f\"\\n\\n{result}\")"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"_cxe5w36L7Wv"},"source":["[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D5_ReinforcementLearningForGames/solutions/W3D5_Tutorial2_Solution_d4eb947f.py)\n","\n"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"tr3rcuIXL7Ww"},"source":["```\n","(14, 6, 0)\n","```"]},{"cell_type":"markdown","metadata":{"execution":{},"id":"8ZgIAR8wL7Ww"},"source":["---\n","# Summary\n","\n","In this tutorial, you have learned about value-based players and compared them to a random player."]},{"cell_type":"code","execution_count":9,"metadata":{"cellView":"form","execution":{},"id":"IMXsyOG8L7Ww","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"ok","timestamp":1659119035227,"user_tz":180,"elapsed":265,"user":{"displayName":"Joao Pedro Carvalho Moreira","userId":"11144433980626047251"}},"outputId":"1848d0b1-9ab0-43fc-9db0-0c32dbfb7478"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n"," <div>\n","   <a href= \"https://portal.neuromatchacademy.org/api/redirect/to/9c55f6cb-cdf9-4429-ac1c-ec44fe64c303?data=eyJmb3JtX2lkIjogImFwcG43VmRQUnNlU29NWEVHIiwgInRhYmxlX25hbWUiOiAiVzNENV9UMiIsICJhbnN3ZXJzIjoge30sICJldmVudHMiOiBbeyJldmVudCI6ICJpbml0IiwgInRzIjogMTY1OTExODkyNi4wMDcxMjg3fSwgeyJldmVudCI6ICJ1cmwgZ2VuZXJhdGVkIiwgInRzIjogMTY1OTExOTAzNS41Njk5MjM5fV19\" target=\"_blank\">\n","   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/AirtableSubmissionButton.png?raw=1\"\n"," alt=\"button link to Airtable\" style=\"width:410px\"></a>\n","   </div>"]},"metadata":{},"execution_count":9}],"source":["# @title Airtable Submission Link\n","from IPython import display as IPydisplay\n","IPydisplay.HTML(\n","   f\"\"\"\n"," <div>\n","   <a href= \"{atform.url()}\" target=\"_blank\">\n","   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/AirtableSubmissionButton.png?raw=1\"\n"," alt=\"button link to Airtable\" style=\"width:410px\"></a>\n","   </div>\"\"\" )"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Cópia de W3D5_Tutorial2","provenance":[{"file_id":"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D5_ReinforcementLearningForGames/student/W3D5_Tutorial2.ipynb","timestamp":1659118818613}]},"kernel":{"display_name":"Python 3","language":"python","name":"python3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}